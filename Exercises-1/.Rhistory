library(tidyverse)
data(mpg)
mpg
# creating a ggplot
# The first line sets up a coordinate system.
# the second line maps displ to x, hwy to y, and draws points
ggplot(data = mpg) +
geom_point(mapping = aes(x = displ, y = hwy))
# mapping color to class
# adding a title by adding another layer
ggplot(mpg, aes(displ, hwy)) +
geom_point(aes(color = class)) +
labs(title = "Fuel efficiency generally decreases with engine size")
# facets
ggplot(data = mpg) +
geom_point(mapping = aes(x = displ, y = hwy)) +
facet_wrap(~ class, nrow = 2)
### Piping/grouping/summarizing
mpg %>%
group_by(class) %>%
summarize(mean.cty = mean(cty))
# Let's calculate average MPG for each model across years
mpg_summ = mpg %>%
group_by(model)  %>%  # group the data points by model nae
summarize(hwy.mean = mean(hwy))  # calculate a mean for each model
# still not in order...
ggplot(mpg_summ, aes(x=model, y=hwy.mean)) +
geom_bar(stat='identity') +
coord_flip()
# reorder the x labels
ggplot(mpg_summ, aes(x=reorder(model, hwy.mean), y=hwy.mean)) +
geom_bar(stat='identity') +
coord_flip()
library(mosaic)
library(tidyverse)
# read in data
creatinine = read.csv("C:/Users/Ian/Documents/UT MA Economics (2019-)/Summer 2019/ECO 394D - Probability and Statistics/Github/data/creatinine.csv", header=TRUE)
# read in data
creatinine = read.csv("C:/Users/imcbr/Documents/UT MA Economics (2019-)/Summer 2019/ECO 394D - Probability and Statistics/Github/data/creatinine.csv", header=TRUE)
# read in data
creatinine = read.csv("C:\Users\imcbr\Box Sync\UT MA Economics (2019-)\Summer 2019\ECO 394D - Probability and Statistics\Github\data", header=TRUE)
# read in data
creatinine = read.csv("C:/Users/imcbr/Box Sync/UT MA Economics (2019-)/Summer 2019/ECO 394D - Probability and Statistics/Github/data", header=TRUE)
library(mosaic)
library(tidyverse)
# read in data
creatinine = read.csv("C:/Users/imcbr/Box Sync/UT MA Economics (2019-)/Summer 2019/ECO 394D - Probability and Statistics/Github/data", header=TRUE)
library(mosaic)
library(tidyverse)
# read in data
creatinine = read.csv("C:/Users/imcbr/Box Sync/UT MA Economics (2019-)/Summer 2019/ECO 394D - Probability and Statistics/Github/data", header=TRUE)
library(mosaic)
library(tidyverse)
# read in data
creatinine = read.csv("C:/Users/imcbr/Box Sync/UT MA Economics (2019-)/Summer 2019/ECO 394D - Probability and Statistics/Github/data", header=TRUE)
# read in data
creatinine = read.csv("C:/Users/imcbr/Box Sync/UT MA Economics (2019-)/Summer 2019/ECO 394D - Probability and Statistics/Github/data/creatinine.csv", header=TRUE)
# simple scatter plot
plot(creatclear ~ age, data = creatinine)
# read in data
creatinine = read.csv("C:/Users/imcbr/Box Sync/UT MA Economics (2019-)/Summer 2019/ECO 394D - Probability and Statistics/Github/data/creatinine.csv", header=TRUE)
# simple scatter plot
plot(creatclear ~ age, data = creatinine)
# fit a linear model
lm2 = lm(creatclear ~ age, data = creatinine)
abline(lm2, col='red')
# look at the coefficients
coef(lm2)
# make prediction on new data
new_data = data.frame(age = c(55,40,60))
predict(lm2, new_data)
# look at the residuals (actual - predicted)
resid(lm2)
# who has the smallest residual? pipe to which.min
resid(lm2) %>% which.min
help(which.min)
library(mosaic)
library(tidyverse)
library(data.table)
#get the data
greenbuildingsuf = read.csv("C:/Users/Ian/Documents/UT MA Economics (2019-)/Summer 2019/ECO 394D - Probability and Statistics/Github/data/greenbuildings.csv", header=TRUE)
#get the data
greenbuildingsuf = read.csv("C:/Users/imcbr/Box Sync/UT MA Economics (2019-)/Summer 2019/ECO 394D - Probability and Statistics/Github/data/greenbuildings.csv", header=TRUE)
#Filter data with zero leasing rate
greenbuildings = subset(greenbuildingsuf,greenbuildingsuf$leasing_rate > 0.01)
#lets make the data a bit better
#Factor necessary columns
greenbuildings$green_rating = as.factor(greenbuildings$green_rating)
levels(greenbuildings$green_rating) = c('Non-Green','Green')
#Create Age Quartiles
greenbuildings$agequartile <- ntile(greenbuildings$age, 4)
#Create Stories Quartiles
greenbuildings$storyquartile <- ntile(greenbuildings$stories, 4)
head(greenbuildings)
summary(greenbuildings$stories)
help(as.factor)
#More Factoring
greenbuildings$storyquartile  = as.factor(greenbuildings$storyquartile )
levels(greenbuildings$storyquartile ) = c('Q1','Q2','Q3','Q4')
summary(greenbuildings$stories)
#More Factoring
greenbuildings$agequartile = as.factor(greenbuildings$agequartile)
levels(greenbuildings$agequartile) = c('Q1','Q2','Q3','Q4')
#Class Factoring
greenbuildings$class_a = as.factor(greenbuildings$class_a)
levels(greenbuildings$class_a) = c('C','A')
greenbuildings$class_b = as.factor(greenbuildings$class_b)
levels(greenbuildings$class_b) = c('C','B')
greenbuildings$Class = paste(as.character(greenbuildings$class_a), as.character(greenbuildings$class_b),sep = '')
greenbuildings$Class = as.factor(greenbuildings$Class)
levels(greenbuildings$Class) = c('A','B','C')
#replicating the faulty analysis
head(greenbuildings)
fdatatf = greenbuildings$leasing_rate > 10
fdata = subset(greenbuildings, fdatatf)
fdataex = subset(greenbuildings, !fdatatf)
summary(fdataex)
summary(subset(fdata$Rent, fdata$green_rating == 'GB'))
summary(subset(fdata$Rent, fdata$green_rating == 'SB'))
#what's wrong with the analysis
plot(Rent ~ leasing_rate, data = greenbuildings)
#Can we assume our building will have a 90% occupancy rate
summary(subset(greenbuildings$leasing_rate, (greenbuildings$green_rating == 'Green' & greenbuildings$agequartile == 'Q1')))
help(data)
library(tidyverse)
data(mpg)
mpg
install.packages("FNN")
library(tidyverse)
library(FNN)
# read in the data: make sure to use the path name to
# wherever you'd stored the file
loadhou = read.csv('../data/loadhou.csv')
summary(loadhou)
# read in the data: make sure to use the path name to
# wherever you'd stored the file
loadhou = read.csv('C:/Users/imcbr/Box Sync/UT MA Economics (2019-)/1-Spring 2020/Data Mining & Statistical Learning - ECO 395M/ScottFiles/ECO395M/data/loadhou.csv')
summary(loadhou)
# plot the data
ggplot(data = loadhou) +
geom_point(mapping = aes(x = KHOU, y = COAST), color='darkgrey') +
ylim(7000, 20000)
# Make a train-test split
N = nrow(loadhou)
N_train = floor(0.8*N)
N_test = N - N_train
# randomly sample a set of data points to include in the training set
train_ind = sample.int(N, N_train, replace=FALSE)
# Define the training and testing set
D_train = loadhou[train_ind,]
D_test = loadhou[-train_ind,]
# optional book-keeping step:
# reorder the rows of the testing set by the KHOU (temperature) variable
# this isn't necessary, but it will allow us to make a pretty plot later
D_test = arrange(D_test, KHOU)
head(D_test)
# Now separate the training and testing sets into features (X) and outcome (y)
X_train = select(D_train, KHOU)
y_train = select(D_train, COAST)
X_test = select(D_test, KHOU)
y_test = select(D_test, COAST)
# linear and quadratic models
lm1 = lm(COAST ~ KHOU, data=D_train)
lm2 = lm(COAST ~ poly(KHOU, 2), data=D_train)
# KNN 250
knn250 = knn.reg(train = X_train, test = X_test, y = y_train, k=250)
names(knn250)
# define a helper function for calculating RMSE
rmse = function(y, ypred) {
sqrt(mean(data.matrix((y-ypred)^2)))
}
ypred_lm1 = predict(lm1, X_test)
ypred_lm2 = predict(lm2, X_test)
ypred_knn250 = knn250$pred
rmse(y_test, ypred_lm1)
rmse(y_test, ypred_lm2)
rmse(y_test, ypred_knn250)
# attach the predictions to the test data frame
D_test$ypred_lm2 = ypred_lm2
D_test$ypred_knn250 = ypred_knn250
p_test = ggplot(data = D_test) +
geom_point(mapping = aes(x = KHOU, y = COAST), color='lightgrey') +
theme_bw(base_size=18) +
ylim(7000, 20000)
p_test
p_test + geom_point(aes(x = KHOU, y = ypred_knn250), color='red')
p_test + geom_path(aes(x = KHOU, y = ypred_knn250), color='red')
p_test + geom_path(aes(x = KHOU, y = ypred_knn250), color='red') +
geom_path(aes(x = KHOU, y = ypred_lm2), color='blue')
N_train = 150
train_ind = sort(sample.int(N, N_train, replace=FALSE))
D_train = loadhou[train_ind,]
D_train = arrange(D_train, KHOU)
y_train = D_train$COAST
X_train = data.frame(KHOU=jitter(D_train$KHOU))
knn_model = knn.reg(X_train, X_train, y_train, k = 3)
D_train$ypred = knn_model$pred
p_train = ggplot(data = D_train) +
geom_point(mapping = aes(x = KHOU, y = COAST), color='lightgrey') +
theme_bw(base_size=18) +
ylim(7000, 20000) + xlim(0,36)
p_train + geom_path(mapping = aes(x=KHOU, y=ypred), color='red', size=1.5)
N_train = 150
train_ind = sort(sample.int(N, N_train, replace=FALSE))
D_train = loadhou[train_ind,]
D_test = loadhou[~train_ind,]
View(D_train)
D_train = arrange(D_train, KHOU)
D_test = arrange(D_test,KHOU)
library(tidyverse)
library(FNN)
# read in the data: make sure to use the path name to
# wherever you'd stored the file
loadhou = read.csv('C:/Users/imcbr/Box Sync/UT MA Economics (2019-)/1-Spring 2020/Data Mining & Statistical Learning - ECO 395M/ScottFiles/ECO395M\data/loadhou.csv')
summary(loadhou)
# plot the data
ggplot(data = loadhou) +
geom_point(mapping = aes(x = KHOU, y = COAST), color='darkgrey') +
ylim(7000, 20000)
# read in the data: make sure to use the path name to
# wherever you'd stored the file
loadhou = read.csv('C:/Users/imcbr/Box Sync/UT MA Economics (2019-)/1-Spring 2020/Data Mining & Statistical Learning - ECO 395M/ScottFiles/ECO395M/data/loadhou.csv')
summary(loadhou)
# plot the data
ggplot(data = loadhou) +
geom_point(mapping = aes(x = KHOU, y = COAST), color='darkgrey') +
ylim(7000, 20000)
N_train = 150
train_ind = sort(sample.int(N, N_train, replace=FALSE))
D_train = loadhou[train_ind,]
# Make a train-test split
N = nrow(loadhou)
N_train = 150
train_ind = sort(sample.int(N, N_train, replace=FALSE))
D_train = loadhou[train_ind,]
D_test = loadhou[~train_ind,]
D_train = arrange(D_train, KHOU)
D_test = arrange(D_test,KHOU)
D_test = loadhou[-train_ind,]
D_train = arrange(D_train, KHOU)
D_test = arrange(D_test,KHOU)
y_train = D_train$COAST
help(jitter)
library(tidyverse)
oj = read.csv("../data/oj.csv")
## read in the data
oj = read.csv("..\data\oj.csv")
## read in the data
oj = read.csv("../data/oj.csv")
## read in the data
oj = read.csv("Data/oj.csv")
levels(oj$brand)
ggplot(data=oj) +
geom_boxplot(aes(x=brand, y = log(price)))
plot(log(price) ~ brand, data=oj)
plot(logmove ~ log(price), data=oj)
## simple regression
reg = lm(logmove ~ log(price) + brand, data=oj)
## use the fitted model
summary(reg) ## coef, tests, fit
map.tokyo <- get_map("Tokyo")
library(Hmisc)
library(tidyverse)
library(FNN)
library(mosaic)
library(ggmap)
abia = read.csv('ABIA.csv')
abia_well_behaved = subset(abia, DepDelay != "NA")
abia_not_well_behaved <- subset(abia, is.na(abia$DepDelay))
describe(abia_not_well_behaved)
#Maybe we should look at something besides the mean to avoid crazy values
#It looks like all the flights that were missing delays were cancelled, but I'm not sure we want to exclude those from analysis
abia_cr<- abia
abia_cr$DepDelay[abia_cr$Cancelled == 1] <- 10000
abia_crd <- subset(abia_cr, Dest != 'AUS')
abia_crd$LongWait <- (abia_crd$DepDelay >= 15)
describe(abia_crd)
abia_sumbydest = abia_crd  %>%
group_by(Dest) %>%
summarize(median.DepDelay = median(DepDelay), FlightCount = length(Dest), Prob_LW = mean(LongWait))
#############
# Exercise 1#
#############
setwd('C:/Users/imcbr/Box Sync/UT MA Economics (2019-)/1-Spring 2020/Data Mining & Statistical Learning - ECO 395M/NathanRepo/DataScienceCourseHomework/Exercises-1')
abia = read.csv('ABIA.csv')
abia_well_behaved = subset(abia, DepDelay != "NA")
abia_not_well_behaved <- subset(abia, is.na(abia$DepDelay))
describe(abia_not_well_behaved)
#Maybe we should look at something besides the mean to avoid crazy values
#It looks like all the flights that were missing delays were cancelled, but I'm not sure we want to exclude those from analysis
abia_cr<- abia
#Maybe we should look at something besides the mean to avoid crazy values
#It looks like all the flights that were missing delays were cancelled, but I'm not sure we want to exclude those from analysis
abia_cr<- abia
abia_cr$DepDelay[abia_cr$Cancelled == 1] <- 10000
abia_crd <- subset(abia_cr, Dest != 'AUS')
abia_crd$LongWait <- (abia_crd$DepDelay >= 15)
describe(abia_crd)
abia_sumbydest = abia_crd  %>%
group_by(Dest) %>%
summarize(median.DepDelay = median(DepDelay), FlightCount = length(Dest), Prob_LW = mean(LongWait))
#Filter groups with not enough data
abia_sumbydest <- subset(abia_sum5, FlightCount >= 50)
view(abia_sumbydest)
#Import Airport codes
codes = read.csv('airport-codes.csv')
codes_f = codes[codes$iata_code %in% abia_sumbydest$Dest,]
view(codes_f)
#join codes
codes_f$Dest = codes_f$iata_code
map.tokyo <- get_map("Tokyo")
map.tokyo <- get_map("Tokyo", source = 'osm')
map.tokyo <- get_map("Tokyo", source = "osm")
map.US_osm <- get_openstreetmap()
map.US_osm <- get_cloudmademap()
help('Defunct')
map.US_osm <- get_navermap()
register_google(key='AIzaSyC95joLfR1EU41l2o4l4pHElo_BM9-oTR8')
map.US <- get_map('United States')
ggmap(map.US)
help(get_map)
testmap <- get_map(source='stamen')
ggmap(testmap)
