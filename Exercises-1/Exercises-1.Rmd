---
title: "Exercises 1"
output: github_document

---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Data Visualization: Flights at ABIA

```{r abia_background, include=FALSE}

library(Hmisc)
library(tidyverse)
library(FNN)
library(mosaic)
library(maps)
library(mapdata)

#read data
abia <- read.csv('ABIA.csv')

#clean & subset
abia_crd <- subset(abia, Dest != 'AUS')
abia_crd$DepDelay[abia_crd$Cancelled == 1] <- 10000
abia_crd$LongWait <- (abia_crd$DepDelay >= 20)

abia_cra <- subset(abia, Dest == 'AUS')
abia_cra$ArrDelay[abia_cra$Cancelled == 1] <- 10000
abia_cra$ArrDelay[abia_cra$Diverted == 1] <- 10000
abia_cra$LongWait <- (abia_cra$ArrDelay >= 20)

abia_sumbydest <- abia_crd  %>%
  group_by(Dest) %>%
  summarize(median.DepDelay = median(DepDelay), FlightCount = length(Dest), Prob_LW = mean(LongWait))

abia_sumbyorig <- abia_cra  %>%
  group_by(Origin) %>%
  summarize(median.ArrDelay = median(ArrDelay), FlightCount = length(Origin), Prob_LW = mean(LongWait))

#Filter groups with not enough data
abia_sumbydest <- subset(abia_sumbydest, FlightCount >= 50)
abia_sumbydest$Prob_LW <- abia_sumbydest$Prob_LW * 100

abia_sumbyorig <- subset(abia_sumbyorig, FlightCount >= 50)
abia_sumbyorig$Prob_LW <- abia_sumbyorig$Prob_LW * 100

#Import Airport codes
codes <- read.csv('airport-codes.csv')
codes <- separate(codes,'coordinates', c('Lat','Long') , sep=',' , remove=TRUE, convert=TRUE)

#filter codes & clean
codes_f <- codes[codes$iata_code %in% abia_sumbydest$Dest,]
codes_f <- separate(codes_f,'iso_region', c('CountryCode','State') , sep='-' , remove=TRUE, convert=TRUE)
codes_f$Dest <- codes_f$iata_code

codes_fa <- codes[codes$iata_code %in% abia_sumbyorig$Origin,]
codes_fa <- separate(codes_fa,'iso_region', c('CountryCode','State') , sep='-' , remove=TRUE, convert=TRUE)
codes_fa$Origin <- codes_fa$iata_code

#merge to grouped data
abia_merged <- merge(abia_sumbydest, codes_f, by='Dest')
abia_merged10 <- abia_merged[with(abia_merged,order(-Prob_LW)),]
abia_merged10 <- abia_merged10[1:10,]
abia_merged9 <- abia_merged10[abia_merged10$Dest != 'JFK',]
abia_mergedJFK <- abia_merged10[abia_merged10$Dest == 'JFK',]

abia_mergeda <- merge(abia_sumbyorig, codes_fa, by='Origin')
abia_merged10a <- abia_mergeda[with(abia_mergeda,order(-Prob_LW)),]
abia_merged10a <- abia_merged10a[1:10,]
abia_merged8a <- abia_merged10a[!(abia_merged10a$Origin %in% c('JFK','RDU')),]
abia_mergedJFKa <- abia_merged10a[abia_merged10a$Origin == 'JFK',]
abia_mergedRDUa <- abia_merged10a[abia_merged10a$Origin == 'RDU',]

##map data
usa <- map_data('usa')
states <- map_data('state')

```

```{r dep_plots, echo = FALSE}

ggUSA <- 
  ggplot() + 
  geom_polygon(data = states, aes(x=long, y = lat, group = group), color = 'white', fill = 'light blue') + 
  coord_fixed(1.3) 

ggUSA + 
  geom_point(data = abia_merged9, aes(x = Long, y = Lat, fill = Prob_LW), shape = 23, size = 5, stroke = 1.75, alpha = .75) +
  geom_point(data = abia_mergedJFK, aes(x = Long+.5, y = Lat, fill = Prob_LW), shape = 23, size = 5, stroke = 1.75, alpha = .75) +
  geom_text(data = abia_merged9, aes(x = Long, y = Lat, label = abia_merged9$Dest ), check_overlap = FALSE, nudge_x = -3.6, size = 5) + 
  geom_text(data = abia_mergedJFK, aes(x = Long, y = Lat, label = abia_mergedJFK$Dest ), check_overlap = FALSE, nudge_x = +4.1, size = 5) + 
  scale_fill_gradient(low = 'yellow', high = 'red',guide = 'colourbar') +
  guides(fill = guide_colorbar(title = '')) +
  ggtitle('10 Worst Destinations for Flights out of Austin', subtitle = 'Percent Chance of a Departure Delay Greater Than 20 Minutes by Destination') + 
  theme(
    axis.text = element_blank(),
    axis.line = element_blank(),
    axis.ticks = element_blank(),
    panel.border = element_blank(),
    panel.grid = element_blank(),
    panel.background = element_blank(),
    axis.title = element_blank(),
    plot.title = element_text(hjust = .5),
    plot.subtitle = element_text(hjust = .5)
  ) 

```

```{r arr_plots, echo= FALSE}

ggUSA_a <- 
  ggplot() + 
  geom_polygon(data = states, aes(x=long, y = lat, group = group), color = 'white', fill = 'light blue') + 
  coord_fixed(1.3) 

ggUSA_a + 
  geom_point(data = abia_merged8a, aes(x = Long, y = Lat, fill = Prob_LW), shape = 23, size = 5, stroke = 1.75, alpha = .75) +
  geom_point(data = abia_mergedJFKa, aes(x = Long+.5, y = Lat, fill = Prob_LW), shape = 23, size = 5, stroke = 1.75, alpha = .75) +
  geom_point(data = abia_mergedRDUa, aes(x = Long, y = Lat, fill = Prob_LW), shape = 23, size = 5, stroke = 1.75, alpha = .75) +
  geom_text(data = abia_merged8a, aes(x = Long, y = Lat, label = abia_merged8a$Origin ), check_overlap = FALSE, nudge_x = -3.4, size = 5) + 
  geom_text(data = abia_mergedJFKa, aes(x = Long, y = Lat, label = abia_mergedJFKa$Origin ), check_overlap = FALSE, nudge_x = +3.9, size = 5) +
  geom_text(data = abia_mergedRDUa, aes(x = Long, y = Lat, label = abia_mergedRDUa$Origin ), check_overlap = FALSE, nudge_x = +3.4, size = 5) +
  scale_fill_gradient(low = 'yellow', high = 'red',guide = 'colourbar') +
  guides(fill = guide_colorbar(title = '')) +
  ggtitle('10 Worst Origination Airports for Flights into Austin', subtitle = 'Percent Chance of an Arrival Delay Greater Than 20 Minutes by Origin') + 
  theme(
    axis.text = element_blank(),
    axis.line = element_blank(),
    axis.ticks = element_blank(),
    panel.border = element_blank(),
    panel.grid = element_blank(),
    panel.background = element_blank(),
    axis.title = element_blank(),
    plot.title = element_text(hjust = .5),
    plot.subtitle = element_text(hjust = .5)
  ) 





```

## K-Nearest Neighbors Model: Car Price

Using the K-nearest neighbors (KNN) model, we examine how mileage relates to price for two trim levels of the Mercedes S Class: the 350, and the 65 AMG. 

For different values of k, KNN models make varying prediction errors given out-of-sample data. The following graphs show the relationship between k-value and root mean square error (RMSE), a measure of prediction error, for the two different trim levels. The graphs identify the k-value for which a KNN model minimizes RMSE with a vertical red line.

```{r optimal_k_values, echo = FALSE}
library(tidyverse)
library(FNN)
library(mosaic)

sclass = read.csv('sclass.csv')

# Focus on 2 trim levels: 350 and 65 AMG

# Trim: 350
sclass350 = subset(sclass, trim == '350')

# Make a train-test split
N_350 = nrow(sclass350)
N_train_350 = floor(0.8*N_350)
N_test_350 = N_350 - N_train_350

# Set seed for random sample for presentation purposes
set.seed(404)

# Randomly sample a set of data points to include in the training set
train_indices_350 = sample.int(N_350, N_train_350, replace=FALSE)

# Define the training data and testing data sets
D_train_350 = sclass350[train_indices_350,]
D_test_350 = sclass350[-train_indices_350,]

# Reorder the rows of the testing set by the "mileage" variable
D_test_350 = arrange(D_test_350, mileage)

# Separate training and testing sets into features (X) and outcome (y)
X_train_350 = select(D_train_350, mileage)
y_train_350 = select(D_train_350, price)
x_test_350 = select(D_test_350, mileage)
y_test_350 = select(D_test_350, price)

# Define a helper function for calculating RMSE
rmse = function(y, ypred) {
  sqrt(mean(data.matrix((y-ypred)^2)))
}

# Fit KNN for each k-value from 3 to size of training data and record out-of-sample RMSE
# NOTE: KNN bugs out for k-value 2
knn_rmse_350 = data.frame()

for(i in 3:N_train_350) {
  knn_350 = knn.reg(train = X_train_350, test = x_test_350, y = y_train_350, k=i)
  ypred_knn_350 = knn_350$pred
  knn_rmse =  rmse(y_test_350, ypred_knn_350)
  knn_rmse_350 = rbind(knn_rmse_350,list("k"=i,"rmse" = knn_rmse))
}

# Plot RMSE vs k-value (with log x-axis) with optimal k-value marked
optimal_k_350 = knn_rmse_350$k[which.min(knn_rmse_350$rmse)]
knn_rmse_plot_350 = ggplot(data = knn_rmse_350) +
  geom_path(aes(x = k, y = rmse), color='black')+
  geom_vline(xintercept = optimal_k_350, color = 'red')+
  scale_x_continuous(trans='log10') +
  ggtitle("RMSE for various KNN k-values, 350 trim", ) +
  xlab("k-value") +
  ylab("Root Mean Square Error")
knn_rmse_plot_350 + annotate(geom="text", x=6, y=18000, label= paste("Optimal k-value = ", optimal_k_350), color="red") 

##############
# Trim: 65 AMG
sclass65AMG = subset(sclass, trim == '65 AMG')

# Make a train-test split
N_65AMG = nrow(sclass65AMG)
N_train_65AMG = floor(0.8*N_65AMG)
N_test_65AMG = N_65AMG - N_train_65AMG

# Randomly sample a set of data points to include in the training set
train_indices_65AMG = sample.int(N_65AMG, N_train_65AMG, replace=FALSE)

# Define the training data and testing data sets
D_train_65AMG = sclass65AMG[train_indices_65AMG,]
D_test_65AMG = sclass65AMG[-train_indices_65AMG,]

# Reorder the rows of the testing set by the "mileage" variable
D_test_65AMG = arrange(D_test_65AMG, mileage)

# Separate training and testing sets into features (X) and outcome (y)
X_train_65AMG = select(D_train_65AMG, mileage)
y_train_65AMG = select(D_train_65AMG, price)
x_test_65AMG = select(D_test_65AMG, mileage)
y_test_65AMG = select(D_test_65AMG, price)


# Fit KNN for each k-value from 3 to size of training data and record out-of-sample RMSE
# NOTE: KNN bugs out for k-value 2
knn_rmse_65AMG = data.frame()

for(i in 3:N_train_65AMG) {
  knn_65AMG = knn.reg(train = X_train_65AMG, test = x_test_65AMG, y = y_train_65AMG, k=i)
  ypred_knn_65AMG = knn_65AMG$pred
  knn_rmse =  rmse(y_test_65AMG, ypred_knn_65AMG)
  knn_rmse_65AMG = rbind(knn_rmse_65AMG,list("k"=i,"rmse" = knn_rmse))
}

# Plot RMSE vs k-value with log x-axis
optimal_k_65AMG = knn_rmse_65AMG$k[which.min(knn_rmse_65AMG$rmse)]
knn_rmse_plot_65AMG = ggplot(data = knn_rmse_65AMG) +
  geom_path(aes(x = k, y = rmse), color='black') + 
  geom_vline(xintercept = optimal_k_65AMG, color = 'red') +
  scale_x_continuous(trans='log10') +
  ggtitle("RMSE for various KNN k-values, 65 AMG trim", ) +
  xlab("k-value") +
  ylab("Root Mean Square Error")
knn_rmse_plot_65AMG + annotate(geom="text", x=20, y=60000, label= paste("Optimal k-value = ", optimal_k_65AMG), color="red") 


``` 

Each of the red lines on the graphs above represents the KNN model that minimizes the RMSE. The graphs that follow show those KNN models plotted against the test data from which the RMSE data was calculated.

```{r optimal_knn_models, echo = FALSE}

# Plot optimal k-value KNN with test data for 350
knn_optimal_350 = knn.reg(train = X_train_350, test = x_test_350, y = y_train_350, k=optimal_k_350)

p_test_350 = ggplot(data = D_test_350) + 
  geom_point(mapping = aes(x = mileage, y = price), color='lightgrey') + 
  theme_bw(base_size=18) + 
  geom_path(aes(x = mileage, y = knn_optimal_350$pred), color='red') +
  labs(title = "Optimal KNN model, 350", subtitle = paste("k =", optimal_k_350), x = "Mileage", y = "Price ($)")
p_test_350

# Plot optimal k-value KNN with test data for 65 AMG
knn_optimal_65AMG = knn.reg(train = X_train_65AMG, test = x_test_65AMG, y = y_train_65AMG, k=optimal_k_65AMG)

p_test_65AMG = ggplot(data = D_test_65AMG) + 
  geom_point(mapping = aes(x = mileage, y = price), color='lightgrey') + 
  theme_bw(base_size=18) + 
  geom_path(aes(x = mileage, y = knn_optimal_65AMG$pred), color='red') +
  labs(title = "Optimal KNN model, 65 AMG", subtitle = paste("k =", optimal_k_65AMG), x = "Mileage", y = "Price ($)")
p_test_65AMG
```

As shown above, the 350's optimal k-value is larger than the 65 AMG's. This may be the case because the sample size for the 350 is larger than the sample size for the 65 AMG, More bias near the tails may be counteracted by the improved predictive performance on larger number of samples nearer the center of the distribution.



