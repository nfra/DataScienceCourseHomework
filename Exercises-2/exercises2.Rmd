---
title: "Exercises 2"
author: "Nathan Franz, Ian McBride, and Claire Roycroft"
output: github_document
---

```{r setup, include=FALSE}
library(tidyverse)
library(mosaic)
library(FNN)
library(ggthemes)
```

```{r libraries, include=FALSE}
library(tidyverse)
library(mosaic)
library(FNN)
library(formula.tools)
library(foreach)
library(stargazer)
```

# Saratoga house prices
```{r Exercise_1, include=FALSE, cache=TRUE}

data(SaratogaHouses)

# define rmse function
rmse = function(y, yhat) {
  sqrt( mean( (y - yhat)^2 ) )
}

# Try creating useful variables
#SaratogaHouses = mutate(SaratogaHouses, otherRooms = rooms - bedrooms)
#SaratogaHouses = mutate(SaratogaHouses, usedSpace = livingArea/(lotSize*43560.04))
#SaratogaHouses = mutate(SaratogaHouses, lnAge = log(age))


rmse_vals_better = do(100)* {

  # splitting
  n = nrow(SaratogaHouses)
  n_train = round(0.8*n)  # round to nearest integer
  n_test = n - n_train
  train_cases = sample.int(n, n_train, replace=FALSE)
  test_cases = setdiff(1:n, train_cases)
  saratoga_train = SaratogaHouses[train_cases,]
  saratoga_test = SaratogaHouses[test_cases,]

  # regressions
  #regression from class
  lm_medium2 = lm(price ~ . - sewer - waterfront - landValue - newConstruction, data=saratoga_train)
  #attempted regressions to decrease RMSE
  lm_better=lm(price ~ . -sewer - waterfront -landValue , data=saratoga_train)
  #interaction of bedrooms,bathrooms, rooms and interaction of living area and new construction
  lm_better_int=lm(price ~ sewer+ bedrooms*bathrooms*rooms +heating +fuel +pctCollege +lotSize + livingArea*newConstruction, data=saratoga_train)
  #drop fuel variable and sewer, add waterfront, landValue, centralAir, and interact age with lotSize
  lm_best=lm(price~ bedrooms*bathrooms*rooms + heating + livingArea*newConstruction + age*lotSize + waterfront + landValue + centralAir
             , data=saratoga_train)
  #lm_best duplicate test
  lm_best2=lm(price ~ bedrooms*bathrooms*rooms + heating + livingArea*newConstruction + age*lotSize + waterfront + landValue + centralAir
              , data=saratoga_train)
  #stepwise selection model
  lm_step =lm(formula = price ~ lotSize + age + livingArea + pctCollege +
            bedrooms + fireplaces + bathrooms + rooms + heating + fuel +
            centralAir + landValue + waterfront + newConstruction + livingArea:centralAir +
            landValue:newConstruction + bathrooms:heating + livingArea:fuel +
            pctCollege:fireplaces + lotSize:landValue + fuel:centralAir +
            age:centralAir + age:pctCollege + livingArea:waterfront +
            fireplaces:waterfront + fireplaces:landValue + livingArea:fireplaces +
            bedrooms:fireplaces + pctCollege:landValue + bedrooms:waterfront +
            bathrooms:landValue + heating:waterfront + rooms:heating +
            bedrooms:heating + rooms:fuel, data = saratoga_train)


  # predictions
  yhat_medium = predict(lm_medium2, saratoga_test)
  yhat_better= predict(lm_better, saratoga_test)
  yhat_better_int=predict(lm_better_int, saratoga_test)
  yhat_best=predict(lm_best, saratoga_test)
  yhat_best2=predict(lm_best2, saratoga_test)
  yhat_step=predict(lm_step, saratoga_test)



  c(rmse(saratoga_test$price, yhat_medium),
    rmse(saratoga_test$price, yhat_better),
    rmse(saratoga_test$price, yhat_better_int),
    rmse(saratoga_test$price, yhat_best),  
    rmse(saratoga_test$price, yhat_best2),
    rmse(saratoga_test$price, yhat_step)
  )
}

colMeans(rmse_vals_better)
which.min(colMeans(rmse_vals_better))
baseline_rmse <- as.numeric(colMeans(rmse_vals_better)[1])
best_rmse <- as.numeric(colMeans(rmse_vals_better)[5])
step_rmse <- as.numeric(colMeans(rmse_vals_better)[6])
#lm_best is the best performer here, and outperforms test 4 from previous attempts

lmcall <- as.character(formula(lm_best))
lmvars <- unlist(strsplit(gsub('price ~','',lmcall), '\\+'))
drop_test <- c(NULL)
for(i in lmvars){
  curvar <- trimws(i)

  # Positive values are good, measures the benefit of including a given variable in the regression
  # (i.e. removing the variable is associated with a an average increase in RMSE of this amount)
  rmse_var_val <- do(50)*{
    # splitting again
    n = nrow(SaratogaHouses)
    n_train = round(0.8*n)  # round to nearest integer
    n_test = n - n_train
    train_cases = sample.int(n, n_train, replace=FALSE)
    test_cases = setdiff(1:n, train_cases)
    saratoga_train = SaratogaHouses[train_cases,]
    saratoga_test = SaratogaHouses[test_cases,]

    # Test best regression plus modified regression with 1 covariate removed
    origmodel <- lm(as.formula(paste0(lmcall)), data = saratoga_train)
    curmodel <- lm(as.formula(paste0(lmcall,'-',curvar)), data=saratoga_train)

    # predictions
    yhat_orig = predict(origmodel, saratoga_test)
    yhat_cur = predict(curmodel, saratoga_test)
    c(-rmse(saratoga_test$price, yhat_orig)+rmse(saratoga_test$price, yhat_cur))
  }
  drop_test <- append(drop_test, paste0(curvar,': ',as.numeric(colMeans(rmse_var_val))))
}

# drop_test
var_entry_list <- NULL
var_value_list <- NULL
i=1
for (ent in drop_test) {
  var_entry <- trimws(unlist(strsplit(drop_test[i],':'))[1])
  var_value <- as.numeric(unlist(strsplit(drop_test[i],':'))[2])
  var_entry_list[i] <- var_entry
  var_value_list[i] <- var_value
  i <- i + 1

}

drop_test_frame <- do.call(rbind, Map(data.frame, Dropped_Term=var_entry_list, RMSE_Impact=var_value_list))
#KNN using the stepwise selected vars
rmse_vals_knn = do(100)* {

  # splitting
  n = nrow(SaratogaHouses)
  n_train = round(0.8*n)  # round
  n_test = n - n_train
  train_cases = sample.int(n, n_train, replace=FALSE)
  test_cases = setdiff(1:n, train_cases)
  saratoga_train = SaratogaHouses[train_cases,]
  saratoga_test = SaratogaHouses[test_cases,]


  #bedrooms*bathrooms*rooms + heating + livingArea*newConstruction + age*lotSize + waterfront + landValue + centralAir
  xtrain = model.matrix(~ livingArea + landValue + bathrooms + waterfront + newConstruction +  heating +
                          lotSize + age + centralAir + rooms + bedrooms -1, data=saratoga_train)
  xtest = model.matrix(~ livingArea + landValue + bathrooms + waterfront + newConstruction +  heating +
                         lotSize + age + centralAir + rooms + bedrooms -1, data=saratoga_test)

  # training and testing set responses
  ytrain = saratoga_train$price
  ytest = saratoga_test$price

  # rescale
  scale_train = apply(xtrain, 2, sd) # calculate std dev for each column
  xtrain_scaled = scale(xtrain, scale = scale_train)
  xtest_scaled = scale(xtest, scale=scale_train)
  # regressions for different values of K
  k_grid = seq(2, 51)
  rmse_grid = foreach(K = k_grid, .combine='c') %do% {
    knn_model_K = knn.reg(xtrain_scaled, xtest_scaled, ytrain, k=K)
    rmse(ytest, knn_model_K$pred)}
}

# colMeans(rmse_vals_knn)
# which.min(colMeans(rmse_vals_knn))
# min(colMeans(rmse_vals_knn))

knn_rmse <- min(colMeans(rmse_vals_knn))                    

KNN_RMSE_Means<-colMeans(rmse_vals_knn)

plot(k_grid, KNN_RMSE_Means)
k_grid
knn_plot <-
  ggplot() +
  geom_point(aes(x = k_grid, y = KNN_RMSE_Means)) +
  geom_hline(yintercept = knn_rmse, color = 'red') +
  geom_text(aes(0,knn_rmse,label = 'Minimum RMSE of KNN', vjust = 1.4, hjust = 0), color = 'red') +
  geom_hline(yintercept = best_rmse, color = 'blue') +
  geom_text(aes(0,best_rmse,label = 'Avg. RMSE of Best Hand-Built OLS', vjust = -.4, hjust = 0), color = 'blue') +
  geom_hline(yintercept = step_rmse, color = 'dark green') +
  geom_text(aes(0,step_rmse,label = 'Avg. RMSE of Best Step-Selected OLS', vjust = -.4, hjust = 0), color = 'dark green')+
  geom_hline(yintercept = baseline_rmse, color = 'dark orange') +
  geom_text(aes(0,baseline_rmse,label = 'Avg. RMSE of Baseline OLS', vjust = -.4, hjust = 0), color = 'dark orange') +
  labs(x='K Values')+
  labs(y='RMSE')+
  ggtitle('KNN Fit on House Price Data') +
  theme_clean() +
  theme(plot.background=element_blank())

knn_plot

```
In order to predict the market values of properties in the Saratoga, NY market, our team developed several different models based on the available data. We then iteratively scored the performance of our models and implemented various alterations to try to improve each model's predictive ability. While we will not delve into many of the technical details of our analysis in this report, it is important to give a brief description of the primary measure that we used to evaluate model performance. This metric, called 'Root Mean-Squared Error' (RMSE), reports an average difference of the value predicted by our model compared to the actual value of the property. The main goal of our work was to minimize this value.

To start, our team ran a linear regression on a selection of covariates that we strongly suspected would have a significant impact on the market value of a given property. This regression served as the baseline for further analysis; different variables and interaction terms were subsequently added or removed, and the predictive accuracy was re-measured according to the RMSE metric. To rank the relative importance of terms in our final hand-built model, we then individually dropped each of the terms from the regression, re-ran it, then compared the model's performance without a given covariate to its performance with the covariate included. This process yielded several mostly intuitive conclusions; the most important variables to include in our model were the underlying land value, the living area interacted with whether a property was newly built, waterfront status, and room characteristics (with interactions). These results can be seen in the table below. Additionally, the RMSE of the original model was lower than each of the models with one excluded term, which verified that all terms in our regression were positively contributing to our goal of reducing the overall RMSE.

```{r e1_table, echo=FALSE, results='asis'}

stargazer(drop_test_frame[order(drop_test_frame$RMSE_Impact, decreasing = TRUE),], summary = FALSE, type = 'html', rownames = FALSE, title = 'RMSE Impact of Excluding Specified Terms in Linear Regression', no.space = TRUE)

```

After we had improved linear regression model as much as possible manually, we also then proceeded to implement an automated stepwise variable selection process, which mechanically considered all pairwise combinations of covariates in the dataset and included all that would be beneficial to model performance. While it didn't reveal any new significant insights, it did slightly improve performance compared to the manually built model.

Lastly, our team implemented a k-nearest neighbors model with the covariates from our best hand-built linear model. While the optimal k-nearest neighbors model significantly outperformed the baseline linear regression model, it didn't outperform the best hand-built linear model or stepwise linear model. This can be seen in the figure below, which shows the average RMSE values across all values of k and also compares them to the RMSE values of the other models we tested. (A technical note: the average RMSE of the linear models and the average RMSE of the k-nearest neighbor models were calculated using different sets of random samples from the overall data. While it is typically not optimal to compare performance across different sets of random samples, this issue is alleviated by averaging across a large number of random samples within each set, which we do in our analysis.)

```{r e1_results, echo=FALSE}
knn_plot
```

For tax-assessing purposes, the main takeaways from our analysis are clear. Obtaining accurate information about land value, basic house characteristics, and geographic location are all key for forming the best predictive models. Even with this information, however, the RMSE values of our best models are still relatively high (on the order of tens of thousands of dollars). It is unclear on if this is an acceptable level of error for the purposes of tax assessment, though the models would likely yield more accurate results if more data can be obtained. At the very least, these models can serve as a general guideline for aiding in assessment, even if they cannot be completely trusted to come up with true market values by themselves.

# A hospital audit

```{r exercise2setup, include=F}
brca = read.csv("./data/brca.csv")
```

We examine the performance of five radiologists in recalling patients who have undergone a mammogram for further diagnostic screening. Obviously, increasing the correct diagnosis cancer rate is important. Less obviously, this screening involves a significant inconvenience and expense for doctors and patients, so reducing the number of cancer-free patients who are recalled should also be a priority.

First, we compare the doctors' recall rates for signs of systematic differences, accounting for variation in relevant patient characteristics. Then, we compare the doctors' recall rates to the rate of cancer diagnosis within a year of the screening mammogram and we look for information the doctors may not be weighing heavily enough.


## Systematic differences in recall rate among radiologists

Significant systematic differences among radiologists' rates of recall, holding observable patient characteristics constant, would constitute an undesirable heterogeneity. If such discrepancies exist, the cardiologists ought to compare their criteria for recalling a patient. Below, we examine whether that is the case.

Using a logistic regression of patient recall on radiologist, age, history of breast surgery, presence of breast cancer symptoms, menopause and hormone therapy status, and breast density level, we can compare the relative conservatism of each of the doctors.

``` {r radiologist_conservatism, warning=F, echo=F}
binomial_recall_model = glm(recall ~ . - cancer, data=brca, family=binomial)

callback_conservatism = data.frame(summary(binomial_recall_model)$coefficients[2:5,])
callback_conservatism$radiologist = c('34', '66', '89', '95')
callback_conservatism$ci95_low = callback_conservatism[,1] - 1.96*callback_conservatism[,2]
callback_conservatism$ci95_high = callback_conservatism[,1] + 1.96*callback_conservatism[,2]

ggplot(aes(x = radiologist,
           y = Estimate),
       data = callback_conservatism) +
  geom_pointrange(aes(ymin = ci95_low, ymax = ci95_high)) +
  geom_hline(yintercept = 0, color = "red") +
  theme_clean() +
  theme(plot.background=element_blank()) +
  xlab('Radiologist Identification Number') +
  ylab('Coefficient Estimate (with 95% CI)')
```

The figure above gives the coefficient estimates and 95% confidence interval (CI) for the radiologists, relative to radiologist 13. The higher the coefficient, the more likely that radiologist was to recall a patient, controlling for the covariates mentioned above; the lower the coefficient, the less likely.

As shown in the figure, none of the doctors' recall rates significantly differ from any other's at the 95% confidence level. However, Radiologist 89 does recall patients significantly more often at the 90% confidence level.

## Systematic error in recall rate

If there are observable patient characteristics that are associated with  significantly increased probability of being diagnosed with breast cancer within twelve months of a diagnostic mammogram, holding rate of recall fixed, then the doctors should give those characteristics more attention. Below, we examine whether this is the case, both for all doctors as a group and for each doctor individually.


### Among all radiologists

We consider a logistic regression of cancer rate on whether the patient was recalled, the diagnosing radiologist, and patient characteristics.

```{r callback_error_group, warning=F, echo=F}
logit_cancer_model = glm(cancer ~ ., data=brca, family=binomial)

callback_error = data.frame(summary(logit_cancer_model)$coefficients)
callback_error_sig = callback_error[callback_error$Pr...z..<=0.1,]
callback_error_sig$label = c('Intercept', 'Recalled', 'Age > 70', 'Tissue Density 4')

callback_error_sig$ci95_low = callback_error_sig[,1] - 1.96*callback_error_sig[,2]
callback_error_sig$ci95_high = callback_error_sig[,1] + 1.96*callback_error_sig[,2]

ggplot(aes(x = label,
           y = Estimate),
       data = callback_error_sig[callback_error_sig$label != 'Intercept',]) +
  theme_clean() +
  theme(plot.background=element_blank()) +
  geom_pointrange(aes(ymin = ci95_low, ymax = ci95_high)) +
  geom_hline(yintercept = 0, color = "red")
```

The radiologists should consider being more willing to recall patients who are 70 or older and who have type 4 tissue density, which both have significantly positive coefficient estimates at the 90% confidence level.

### For each radiologist

We consider logistic regressions of cancer rate on whether the patient was recalled and patient characteristics for each individual radiologist. The plots below show the coefficient estimates that are significant at the 90% level. 

```{r callback_error_radiologist13, warning=F, echo=F}
# Radiologist 13
logit_cancer13_model = glm(cancer ~ . , data=brca[brca$radiologist == 'radiologist13', 2:8], family=binomial)

callback_error_13 = data.frame(summary(logit_cancer13_model)$coefficients)
callback_error_13_sig = callback_error_13[callback_error_13$Pr...z..<=0.1,]
callback_error_13_sig$label = c('Recalled', 'Age 50-59')

callback_error_13_sig$ci95_low = callback_error_13_sig[,1] - 1.96*callback_error_13_sig[,2]
callback_error_13_sig$ci95_high = callback_error_13_sig[,1] + 1.96*callback_error_13_sig[,2]

ggplot(aes(x = label,
           y = Estimate),
       data = callback_error_13_sig) +
  geom_pointrange(aes(ymin = ci95_low, ymax = ci95_high)) +
  geom_hline(yintercept = 0, color = "red") +
  theme_clean() +
  theme(plot.background=element_blank()) +
  xlab('') +
  ylab('Coefficient Estimate (with 95% CI)')
```

Radiologist 13 should consider being more willing to recall patients between 50 and 59 years old, which is significant at the 90% confidence level.

```{r callback_error_radiologist34, warning=F, echo=F}
# Radiologist 34
logit_cancer34_model = glm(cancer ~ . , data=brca[brca$radiologist == 'radiologist34', 2:8], family=binomial)

callback_error_34 = data.frame(summary(logit_cancer34_model)$coefficients)
callback_error_34_sig = callback_error_34[callback_error_34$Pr...z..<=0.1,]
callback_error_34_sig$label = c('Intercept', 'Age 50-59', 'Breast Surgery History', 'Post Meno, HT Unk')

callback_error_34_sig$ci95_low = callback_error_34_sig[,1] - 1.96*callback_error_34_sig[,2]
callback_error_34_sig$ci95_high = callback_error_34_sig[,1] + 1.96*callback_error_34_sig[,2]

ggplot(aes(x = label,
           y = Estimate),
       data = callback_error_34_sig[callback_error_34_sig$label != 'Intercept',]) +
  geom_pointrange(aes(ymin = ci95_low, ymax = ci95_high)) +
  geom_hline(yintercept = 0, color = "red") +
  theme_clean() +
  theme(plot.background=element_blank()) +
  xlab('') +
  ylab('Coefficient Estimate (with 95% CI)')
```

Radiologist 34 should consider being more willing to recall patients between 50 and 59 years old and patients who have had a breast surgery or biopsy, both of which are significant at the 95% confidence level.

```{r callback_error_radiologist66, warning=F, echo=F}
# Radiologist 66
logit_cancer66_model = glm(cancer ~ . , data=brca[brca$radiologist == 'radiologist66', 2:8], family=binomial)

callback_error_66 = data.frame(summary(logit_cancer66_model)$coefficients)
callback_error_66_sig = callback_error_66[callback_error_66$Pr...z..<=0.1,]
callback_error_66_sig$label = c('Recalled', 'Age 70+', 'Pre Meno')

callback_error_66_sig$ci95_low = callback_error_66_sig[,1] - 1.96*callback_error_66_sig[,2]
callback_error_66_sig$ci95_high = callback_error_66_sig[,1] + 1.96*callback_error_66_sig[,2]

ggplot(aes(x = label,
           y = Estimate),
       data = callback_error_66_sig) +
  geom_pointrange(aes(ymin = ci95_low, ymax = ci95_high)) +
  geom_hline(yintercept = 0, color = "red") +
  theme_clean() +
  theme(plot.background=element_blank()) +
  xlab('') +
  ylab('Coefficient Estimate (with 95% CI)')
```

Radiologist 66 should consider being less willing to recall patients 70 or older or who are premenopausal, both of which are significant at the 90% confidence level.

```{r callback_error_radiologist89, warning=F, echo=F}
# Radiologist 89
logit_cancer89_model = glm(cancer ~ . , data=brca[brca$radiologist == 'radiologist89', 2:8], family=binomial)

callback_error_89 = data.frame(summary(logit_cancer89_model)$coefficients)
callback_error_89_sig = callback_error_89[callback_error_89$Pr...z..<=0.1,]
callback_error_89_sig$label = c('Recalled', 'BC Symptoms')

callback_error_89_sig$ci95_low = callback_error_89_sig[,1] - 1.96*callback_error_89_sig[,2]
callback_error_89_sig$ci95_high = callback_error_89_sig[,1] + 1.96*callback_error_89_sig[,2]

ggplot(aes(x = label,
           y = Estimate),
       data = callback_error_89_sig) +
  geom_pointrange(aes(ymin = ci95_low, ymax = ci95_high)) +
  geom_hline(yintercept = 0, color = "red") +
  theme_clean() +
  theme(plot.background=element_blank()) +
  xlab('') +
  ylab('Coefficient Estimate (with 95% CI)')
```

Radiologist 89 should consider being more willing to recall patients who are exhibiting symptoms of breast cancer, which is significant at the 95% confidence level.

```{r cancer_error_radiologist95, warning=F, echo=F}
# Radiologist 95
logit_cancer95_model = glm(cancer ~ . , data=brca[brca$radiologist == 'radiologist95', 2:8], family=binomial)

callback_error_95 = data.frame(summary(logit_cancer95_model)$coefficients)
callback_error_95_sig = callback_error_95[callback_error_95$Pr...z..<=0.1,]
callback_error_95_sig$label = c('Recalled')

callback_error_95_sig$ci95_low = callback_error_95_sig[,1] - 1.96*callback_error_95_sig[,2]
callback_error_95_sig$ci95_high = callback_error_95_sig[,1] + 1.96*callback_error_95_sig[,2]

ggplot(aes(x = label,
           y = Estimate),
       data = callback_error_95_sig) +
  geom_pointrange(aes(ymin = ci95_low, ymax = ci95_high)) +
  geom_hline(yintercept = 0, color = "red") +
  theme_clean() +
  theme(plot.background=element_blank()) +
  xlab('') +
  ylab('Coefficient Estimate (with 95% CI)')
```

Radiologist 95 has no clear way to improve his or her recall habits.

# Predicting when articles go viral

```{r problem3setup, include=FALSE, message=FALSE, warning=FALSE}
news<-read_csv("./data/online_news.csv")
news$url<- NULL
```

In order to determine the best approach, several models were built. First a linear model of shares on explanatory variables was run, and then the results were classified as viral or not viral. Two models were built using this method using stepwise selection. Stepwise selection was used for both a linear model with interactions, and one without interactions, and the model without interactions was chosen in both cases because the interactions did not significantly improve the performance of the model. The first was a linear model of shares, and the second was a linear model of log of shares. The log transformation was used to reduce the impact of outliers on the regression. The linear model of log of shares was more successful. Over multiple train test splits, it achieved a lower overall error rate, a lower false positive rate, and a higher true positive rate. This makes sense because the nature of viral articles (number of shares) is likely not linear. This model outperformed the best baseline model which assumes all articles are not viral. 

```{r include=FALSE, message=FALSE, warning=FALSE}
#split data
# splitting
n = nrow(news)
n_train = round(0.8*n)  # round to nearest integer
n_test = n - n_train
train_cases = sample.int(n, n_train, replace=FALSE)
test_cases = setdiff(1:n, train_cases)
news_train = news[train_cases,]
news_test = news[test_cases,]
#out of sample performance

lm_news=lm(shares~data_channel_is_world + data_channel_is_entertainment + 
                 is_weekend + data_channel_is_bus + num_hrefs + self_reference_avg_sharess + 
                 data_channel_is_tech + average_token_length + data_channel_is_lifestyle + 
                 num_imgs + num_self_hrefs + avg_negative_polarity + num_keywords + 
                 title_sentiment_polarity + weekday_is_friday + weekday_is_monday + 
                 title_subjectivity + min_positive_polarity + avg_positive_polarity + 
                 num_videos + data_channel_is_socmed + n_tokens_title +n_tokens_content +num_hrefs
               + average_token_length +global_rate_positive_words +global_rate_negative_words +abs_title_sentiment_polarity, data=news_train)

#log of shares built stepwise
lm_news1=lm(log(shares) ~ data_channel_is_world + data_channel_is_entertainment + 
                 is_weekend + data_channel_is_bus + num_hrefs + self_reference_avg_sharess + 
                 data_channel_is_tech + average_token_length + data_channel_is_lifestyle + 
                 num_imgs + num_self_hrefs + avg_negative_polarity + num_keywords + 
                 title_sentiment_polarity + weekday_is_friday + weekday_is_monday + 
                 title_subjectivity + min_positive_polarity + avg_positive_polarity + 
                 num_videos + data_channel_is_socmed + n_tokens_title +n_tokens_content +num_hrefs
               + average_token_length +global_rate_positive_words +global_rate_negative_words +abs_title_sentiment_polarity, data=news_train)

#logit model
news1<-
  news %>% mutate(viral=ifelse(shares>1400, 1, 0))

glm_news = glm(viral ~ data_channel_is_world + data_channel_is_entertainment + 
                 is_weekend + data_channel_is_bus + num_hrefs + self_reference_avg_sharess + 
                 data_channel_is_tech + average_token_length + data_channel_is_lifestyle + 
                 num_imgs + num_self_hrefs + avg_negative_polarity + num_keywords + 
                 title_sentiment_polarity + weekday_is_friday + weekday_is_monday + 
                 title_subjectivity + min_positive_polarity + avg_positive_polarity + 
                 num_videos + data_channel_is_socmed + n_tokens_title +n_tokens_content +num_hrefs
               + average_token_length +global_rate_positive_words +global_rate_negative_words +abs_title_sentiment_polarity, data=news1, family='binomial')

#confusion matrix for logit model
n = nrow(news1)
n_train = round(0.8*n)  # round to nearest integer
n_test = n - n_train
train_cases = sample.int(n, n_train, replace=FALSE)
test_cases = setdiff(1:n, train_cases)
news1_train = news1[train_cases,]
news1_test = news1[test_cases,]

phat_test_news_logit = predict(glm_news, news1_test, type= 'response')
yhat_test_news_logit = ifelse(phat_test_news_logit > 0.5, 1, 0)
confusion_out_logit = table(y = news1_test$viral, yhat = yhat_test_news_logit)

```

```{r sample confusion matrix, echo=FALSE, message=FALSE, warning=FALSE}
confusion_out_logit
```

```{r building functions for do loops, warning=FALSE, message=FALSE, include=FALSE}
#function for classification for linear models
confusion_fun= function(y, ypred) {
  ifelse(ypred>1400 & y>1400, "truepos", 
         ifelse(ypred>1400 & y<=1400, "falsepos", 
                ifelse(ypred<=1400 & y>1400, "falseneg", "trueneg")))}

#function for classification for logit models
confusion_fun1= function(y, ypred) {
  ifelse(ypred  > .5 & y >.5, "truepos", 
         ifelse(ypred >.5 & y <=.5 , "falsepos", 
                ifelse(ypred <=.5 & y > .5, "falseneg",  "trueneg")))}
```

```{r baseline comparisons, warning=FALSE, message=FALSE, include=FALSE}
#baseline models to compare to
#best model is nonviral
table(news1_test$viral)

baseline_nonviral= do(100)*{# splitting
  n = nrow(news)
  n_train = round(0.8*n)  # round to nearest integer
  n_test = n - n_train
  train_cases = sample.int(n, n_train, replace=FALSE)
  test_cases = setdiff(1:n, train_cases)
  news_train = news[train_cases,]
  news_test = news[test_cases,]
  
  #output
  
  outcome_1=confusion_fun(news_test$shares, 1300)
  
  c(count(outcome_1=="trueneg"),
    count(outcome_1=="falseneg"),
    count(outcome_1=="falsepos"), 
    count(outcome_1=="truepos")
  )
}

colnames(baseline_nonviral) = c("0 0", "1 0", 
                                "0 1", "1 1")
colMeans(baseline_nonviral)
baseline_nvmatrix<-baseline_nonviral %>% 
  pivot_longer(cols = colnames(.)) %>% 
  separate(name, into = c("Value", "Type")) %>% 
  pivot_wider(id_cols = "Value", names_from = "Type",
              values_fn = list(value = mean))
baseline_m<-as.data.frame(baseline_nvmatrix)
```

```{r baseline nonviral matrix,  results = 'asis', echo=FALSE}
stargazer(baseline_m, summary=FALSE, rownames=FALSE, type='html', title="Baseline Confusion Matrix", notes="Values are averaged over 100 train-test splits. Predicted values are columns, and actual values are rows. This baseline model guesses that all articles are not viral.")
```

The baseline model has an overall error rate and false positive rate of approximately 0.49 and a true positive rate of zero over 100 train/test splits. 

```{r Shares model, warning=FALSE, message=FALSE, include=FALSE}
#Shares model loop
shares_model= do(100)*{# splitting
  n = nrow(news)
  n_train = round(0.8*n)  # round to nearest integer
  n_test = n - n_train
  train_cases = sample.int(n, n_train, replace=FALSE)
  test_cases = setdiff(1:n, train_cases)
  news_train = news[train_cases,]
  news_test = news[test_cases,]
  
  # regressions
  lm_news=lm(shares~data_channel_is_world + data_channel_is_entertainment + 
                 is_weekend + data_channel_is_bus + num_hrefs + self_reference_avg_sharess + 
                 data_channel_is_tech + average_token_length + data_channel_is_lifestyle + 
                 num_imgs + num_self_hrefs + avg_negative_polarity + num_keywords + 
                 title_sentiment_polarity + weekday_is_friday + weekday_is_monday + 
                 title_subjectivity + min_positive_polarity + avg_positive_polarity + 
                 num_videos + data_channel_is_socmed + n_tokens_title +n_tokens_content +num_hrefs
               + average_token_length +global_rate_positive_words +global_rate_negative_words +abs_title_sentiment_polarity , data=news_train)
  
  # predictions
  yhat_news1 = predict(lm_news, news_test)
  
  
  #output
  
  outcome_1=confusion_fun(news_test$shares, yhat_news1)
  
  
  c(count(outcome_1=="trueneg"),
    count(outcome_1=="falseneg"),
    count(outcome_1=="falsepos"), 
    count(outcome_1=="truepos")
  )
}
#name columns
colnames(shares_model) = c("0 0", "1 0", 
                           "0 1", "1 1")
#turn into confusion matrix
shares_matrix <-shares_model %>% 
  pivot_longer(cols = colnames(.)) %>% 
  separate(name, into = c("Value", "Type")) %>% 
  pivot_wider(id_cols = "Value", names_from = "Type",
              values_fn = list(value = mean))
shares_matrix
```

```{r log of shares model loop, warning=FALSE, message=FALSE, include=FALSE}
#Log of shares model loop

logshares_model= do(100)*{# splitting
  n = nrow(news)
  n_train = round(0.8*n)  # round to nearest integer
  n_test = n - n_train
  train_cases = sample.int(n, n_train, replace=FALSE)
  test_cases = setdiff(1:n, train_cases)
  news_train = news[train_cases,]
  news_test = news[test_cases,]
  
  # regressions
  lm_news=lm(log(shares) ~ data_channel_is_world + data_channel_is_entertainment + 
               is_weekend + data_channel_is_bus + num_hrefs + self_reference_avg_sharess + 
               data_channel_is_tech + average_token_length + data_channel_is_lifestyle + 
               num_imgs + num_self_hrefs + avg_negative_polarity + num_keywords + 
               title_sentiment_polarity + weekday_is_friday + weekday_is_monday + 
               title_subjectivity + min_positive_polarity + avg_positive_polarity + 
               num_videos + data_channel_is_socmed + n_tokens_title +n_tokens_content +num_hrefs
             + average_token_length +global_rate_positive_words +global_rate_negative_words +abs_title_sentiment_polarity, data=news_train)
  
  # predictions
  yhat_news1 = exp(predict(lm_news, news_test))
  
  
  #output
  
  outcome_1=confusion_fun(news_test$shares, yhat_news1)
  
  
  c(count(outcome_1=="trueneg"),
    count(outcome_1=="falseneg"),
    count(outcome_1=="falsepos"), 
    count(outcome_1=="truepos")
  )
}
#name columns
colnames(logshares_model) = c("0 0", "1 0", 
                           "0 1", "1 1")
#turn into confusion matrix
logshares_matrix<- logshares_model %>% 
  pivot_longer(cols = colnames(.)) %>% 
  separate(name, into = c("Value", "Type")) %>% 
  pivot_wider(id_cols = "Value", names_from = "Type",
              values_fn = list(value = mean))

logshares_m<-as.data.frame(logshares_matrix)
```

```{r log of shares matrix over 100 reps, echo=FALSE,  results = 'asis'}
stargazer(logshares_m, summary=FALSE, rownames=FALSE, type='html', title="Log(Shares) Confusion Matrix", notes="Values are averaged over 100 train-test splits. Predicted values are columns, and actual values are rows.")
```

The log linear model had, averaging over 100 train/test splits, an approximate overall error rate = 0.41, a false positive rate = 0.67, and a true positive rate = 0.86. 

```{r Logit Loop, warning=FALSE, message=FALSE, include=FALSE}
#Logit Loop
logit_model= do(100)*{# splitting
  n = nrow(news1)
  n_train = round(0.8*n)  # round to nearest integer
  n_test = n - n_train
  train_cases = sample.int(n, n_train, replace=FALSE)
  test_cases = setdiff(1:n, train_cases)
  news1_train = news1[train_cases,]
  news1_test = news1[test_cases,]

  # regressions model built stepwise 
  glm_news=glm(viral ~ data_channel_is_world + data_channel_is_entertainment + 
                 is_weekend + data_channel_is_bus + num_hrefs + self_reference_avg_sharess + 
                 data_channel_is_tech + average_token_length + data_channel_is_lifestyle + 
                 num_imgs + num_self_hrefs + avg_negative_polarity + num_keywords + 
                 title_sentiment_polarity + weekday_is_friday + weekday_is_monday + 
                 title_subjectivity + min_positive_polarity + avg_positive_polarity + 
                 num_videos + data_channel_is_socmed + n_tokens_title +n_tokens_content +num_hrefs
               + average_token_length +global_rate_positive_words +global_rate_negative_words +abs_title_sentiment_polarity
               ,data=news1_train, family='binomial')
  
  # predictions
  yhat_newsglm = predict(glm_news, news1_test, type='response')
  
  #output
  
  outcome_2=confusion_fun1(news1_test$viral, yhat_newsglm)
  
  
  c(count(outcome_2=="trueneg"),
    count(outcome_2=="falseneg"),
    count(outcome_2=="falsepos"), 
    count(outcome_2=="truepos")
  )
}
#name columns

colnames(logit_model) = c("0 0", "1 0", 
                          "0 1", "1 1")
#turn into confusion matrix
logit_matrix<- logit_model %>% 
  pivot_longer(cols = colnames(.)) %>% 
  separate(name, into = c("Value", "Type")) %>% 
  pivot_wider(id_cols = "Value", names_from = "Type",
              values_fn = list(value = mean))
logit_m<-as.data.frame(logit_matrix)
logit_matrix

```

```{r logit model confusion matrix averages, echo=FALSE , results = 'asis'}
stargazer(logit_m, summary=FALSE, rownames=FALSE, type='html', title="Logit Confusion Matrix", notes="Values are averaged over 100 train-test splits. Predicted values are columns, and actual values are rows.")
```

The second approach was to create an indicator variable for whether or not each article was viral based on its shares. To determine the best model to use for a logit regression, two methods were used. The first was a stepwise model, and the second was a lasso model. There were no major differences between the performance of these models. The logit model using stepwise variable selection outperformed the linear models and the best baseline model because it has more of an increase in true positive rate than false positive rate compared to the log linear model. The logit model had, averaging over 100 train/test splits, an approximate overall error rate = 0.37, a false positive rate = 0.37, and a true positive rate = 0.63.

The first method is effectively using a linear regression to predict a binary outcome. Logit performs better for classification problems like this because it is designed for binary classification using maximum likelihood estimation, while linear models use ordinary least squares, which may be less efficient in this case in terms of the true positive rate is lower. In the case of the linear model, the model is more likely to guess viral, because of the outliers (this was even more apparent in the model for shares). The overall error rate for the logit model is lower, and the false positive rate outperforms the baseline and the linear model. However, if the true positive rate were very important in determining a model's performance, the linear model would be better. 




